{"cells":[{"cell_type":"markdown","metadata":{"id":"lZULiNydi-9t"},"source":["## Reference sources:\n","- https://www.dataquest.io/blog/pandas-cheat-sheet/\n","- Additional info also from my notes from IBM Data Science course\n","- Complete python 3 bootcamp\n","- https://pandas.pydata.org/pandas-docs/stable/reference/index.html\n","- DV0101EN-1-1-1-Introduction-to-Matplotlib-and-Line-Plots-py-v2.0\n","- exploratory-data-analysis\n","- ML0101EN-RecSys-Collaborative-Filtering-movies-py-v1\n","- ML0101EN-RecSys-Content-Based-movies-py-v1\n"]},{"cell_type":"markdown","metadata":{"id":"nrAnGoMDi-90"},"source":["### Importing Data\n","#### Use these commands to import data from a variety of different sources and formats."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OqGM9HiLi-91"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","pd.read_csv(filename) # From a CSV file\n","df = pd.read_csv(filename, names = headers) #create a variable called headers and use names function for column names\n","pd.read_table(filename) # From a delimited text file (like TSV)\n","pd.read_excel(filename) # From an Excel file\n","dfsummary = pd.read_excel(\"Davenport'sRecipeBatches.xlsx\", sheet_name=None) # ordered dic of all sheets\n","pd.read_sql(query, connection_object) # Read from a SQL table/database\n","pd.read_json(json_string) # Read from a JSON formatted string, URL or file.\n","pd.read_html(url) # Parses an html URL, string or file and extracts tables to a list of dataframes\n","pd.read_clipboard() # Takes the contents of your clipboard and passes it to read_table()\n","pd.DataFrame(dict) # From a dict, keys for columns names, values for data as lists\n","df=df._get_numeric_data() #only produce numeric data"]},{"cell_type":"markdown","metadata":{"id":"fTmEnZvZi-93"},"source":["### Standard clean up and data manipulation script"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1RFWCJIVi-93"},"outputs":[],"source":["###Use the below as appropriate for starting a dataframe and inspection###\n","#import pandas as pd\n","#missing_values = [\"n/a\", \"na\", \"--\"]\n","#file = pd.read_csv(\"property_data.csv\", na_values = missing_values)\n","#df = pd.DataFrame(\"file\")\n","#df.shape\n","#df.head(5)\n","#df.info()\n","#df.describe()\n","#df.isnull().values.any()\n","#df.isnull().values.all()\n","#df.isnull().sum()\n","#df.duplicated().sum()\n","#df.columns.values\n","#df.columns.unique() #to check\n","#df.nunique() #number of unique values\n","#df.index.values\n","#df.corr()\n","#grouped_test2.get_group('4wd')['price'] # obtains values (not dataframe) of grouped data\n","#df.pivot_table(index=col1,values=[col2,col3],aggfunc=mean) # Create a pivot table that groups by col1 \n","#to detect numbers:\n","\"\"\"\n","count=0\n","for row in df['OWN_OCCUPIED']:\n","    try:\n","        int(row)\n","        df.loc[count, 'OWN_OCCUPIED']=np.nan\n","    except ValueError:\n","        pass\n","    count+=1\n","\"\"\"\n","\n","###Use the below as appropriate for data manipulation###\n","#df.sort_values(\"col1\")\n","#df.groupby([\"col1\",\"col2\"])\n","#df.dropna(how=any)\n","#df.fillna()\n","#df.replace(this, withthat)\n","#df.at(index, col, value)\n","#df.iloc[1:4]\n","#df.loc[\"d\",\"score\"] = 11.5 #specify string index row and string column\n","#df['Col1'].loc[(df['Col1'] < 90)] = variable\n","#print(df['col3']>0)\n","#print(df['col1'].isin([\"Item1\"]))\n","#df[df['col1'].between(15, 20)]\n","#df[(df['col3']>0) & (df['col1'].isin([\"Item1\"]))]\n","#df[df.col2.str.contains(\"word\")]\n","#df.iat[1,1]=\"new word\"\n","#df = df.drop(\"col1\", axis=1)\n","#df[\"new col\"] = list\n","#df['due_date'] = pd.to_datetime(df['due_date']) #converts to datetime object\n","#df.rename(columns={'old_name': 'new_ name'}) # Selective renaming\n","#df.explode('Col1')\n","#pd.qcut(df['Col1'], q=10) #cuts a continuous value column into bins of n=q\n","\n","#from sklearn.preprocessing import LabelEncoder\n","#le = preprocessing.LabelEncoder()\n","#print(df.head())\n","#for column in df.columns:\n","#    df[column] = le.fit_transform(df[column].astype(\"string\"))\n","#print(df.head())\n","\n","#df1.append(df2) # Add the rows in df2 to the end of df1? (columns should be identical)\n","#pd.concat([df1, df2],axis=1) # Add the columns in df1 to the end of df2 (rows should be identical)\n","#inputMovies = pd.merge(inputId, inputMovies) # merging two df of equal length"]},{"cell_type":"markdown","metadata":{"id":"0L9IUBoCi-99"},"source":["### Exporting Data\n","#### Use these commands to export a DataFrame to CSV, .xlsx, SQL, or JSON."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q6CHyvDHi-99"},"outputs":[],"source":["df.to_csv(filename) # Write to a CSV file\n","df.to_excel(filename) # Write to an Excel file\n","df.to_sql(table_name, connection_object) # Write to a SQL table\n","df.to_json(filename) # Write to a file in JSON format"]},{"cell_type":"markdown","metadata":{"id":"m8PpgLf6i-9-"},"source":["### Create Test Objects\n","#### These commands can be useful for creating test segments."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"23NUhiiCi-9_"},"outputs":[],"source":["pd.DataFrame(np.random.rand(20,5)) # 5 columns and 20 rows of random floats\n","pd.Series(my_list) # Create a series from an iterable my_list\n","df.index = pd.date_range('1900/1/30', periods=df.shape[0]) # Add a date index\n","#Create a dataframe from lists, into a dictionary, into a dataframe\n","Summary_dataframe = {'Algorithmtypes':Algorithmtypes,'Jaccard':JaccardScores,'F1-Score':F1Scores ,'LogLoss': LogLossScores}\n","Summary = pd.DataFrame(Summary_dataframe)"]},{"cell_type":"markdown","metadata":{"id":"hgrBR3nYi-9_"},"source":["### Viewing/Inspecting Data\n","#### Use these commands to take a look at specific sections of your pandas DataFrame or Series."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rhwUW63mi--A"},"outputs":[],"source":["df.head(n) # First n rows of the DataFrame\n","df.tail(n) # Last n rows of the DataFrame\n","df.shape # Number of rows and columns\n","df.info() # Index, Datatype and Memory information\n","df.columns.values #values in columns\n","df.index.values #values in index\n","df.describe() # Summary statistics for numerical columns\n","s.value_counts(dropna=False) # View unique values and counts\n","df.apply(pd.Series.value_counts) # Unique values and counts for all columns\n","df.dtypes #the datatypes for each column\n","all(isinstance(column, str) for column in df.columns) # boolean result to check if all column labels are strings\n","df['drive-wheels'].value_counts().to_frame() # creates data frame for column values"]},{"cell_type":"markdown","metadata":{"id":"8xLQSZ76i--A"},"source":["### Selection\n","#### Use these commands to select a specific subset of your data."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N4D47N3Ji--B"},"outputs":[],"source":["df[col] # Returns column with label col as Series due to single square brackets\n","df[[col1, col2]] # Returns columns as a new DataFrame due to double squares\n","s.iloc[0] # Selection by position\n","s.loc['index_one'] # Selection by index\n","df.iloc[0,:] # First row\n","df.iloc[0,0] # First element of first column\n","dfi_temp = dfi.loc[dfi['ingredient'] == ingredients_dropdown.value # select a row based on a column value\n","df.loc['Column_name'] # Select based on name\n","df[(df['Column_1']=='Value1') & (df['Column2']=='Value2')] # select data based on more than one condition from multiple columns"]},{"cell_type":"markdown","metadata":{"id":"X0tDrJHKi--B"},"source":["### Data Cleaning\n","#### Use these commands to perform a variety of data cleaning tasks."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ddE7X5Cni--B"},"outputs":[],"source":["df.columns = ['a','b','c'] # Rename columns\n","df.columns = headers #where headers is a variable list of headers\n","pd.isnull() # Checks for null Values, Returns Boolean Arrray. could use .sum() as well\n","pd.notnull() # Opposite of pd.isnull()\n","df.dropna() # Drop all rows that contain null values\n","df.dropna(axis=1) # Drop all columns that contain null values\n","df.dropna(axis=1,thresh=n) # Drop all rows have have less than n non null values\n","df.dropna(subset=[\"price\"], axis=0) #drop missing values along the column \"price\"  \n","df.drop(['Col1','col2','etc'], axis=1, inplace=True) #drop columns from dataframe\n","df.drop('Col1',1) # test if this shorthand works\n","df.fillna(x) # Replace all null values with x\n","df.columns = list(map(str, df.columns)) # convert numbered int columns into strings\n","s.fillna(s.mean()) # Replace all null values with the mean (mean can be replaced with most functions from the statistics module)\n","s.astype(float) # Convert the datatype of the series to float\n","movies_df['year'] = movies_df.title.str.extract('(\\(\\d\\d\\d\\d\\))',expand=False) #create column with part of string from another\n","movies_df['title'] = movies_df['title'].apply(lambda x: x.strip()) #.strip() to get rid of any ending whitespace characters\n","s.replace(1,'one') # Replace all values equal to 1 with 'one'\n","s.replace([1,3],['one','three']) # Replace all 1 with 'one' and 3 with 'three'\n","df.replace(\"?\", np.nan, inplace = True) #replace nan data with ?\n","df['Gender'].replace(to_replace=['male','female'], value=[0,1],inplace=True) #replace specified categories with specified numbers\n","df.rename(columns=lambda x: x + 1) # Mass renaming of columns\n","df.rename(columns={'old_name': 'new_ name'}) # Selective renaming\n","df.set_index('column_one') # Change the index\n","df.rename(index=lambda x: x + 1) # Mass renaming of index\n","drive_wheels_counts.index.name = 'drive-wheels'# direct renaming of index\n","df['due_date'] = pd.to_datetime(df['due_date']) #converts to datetime object\n","df_with_dummies = pd.get_dummies(df, columns=['Gender', 'Smoker']) #one-hot encoding, numerises categories (only binary?)\n","df['length'] = df['length']/df['length'].max() #manually normalising the data\n","df['Total'] = df.sum(axis=1) #sum up the rows in total and create a total column\n","userProfile = userGenreTable.transpose().dot(inputMovies['rating']) #.dot() product, weighting from 2 scalars here, \n","#users*reviews=weights. a  .dot() is multiplying of arrays\n","\n","#for loop through rows in a column to perform a function - here a number is changed to % and rounded to 2dp.\n","row = 0\n","column = 0\n","for a in df1['Very interested']:\n","    b = (100/2233)*a\n","    b = round(b, 2)\n","    location = df1.iloc[row,column]\n","    df1.replace({'Very interested':{location:b}}, inplace=True)\n","    row = row+1\n","\n","#alternative for loop where value counts are given \n","for column in missing_data.columns.values.tolist():\n","    print(column)\n","    print (missing_data[column].value_counts())\n","\n","#quick conversion to another number e.g. temp in C or F or mpg vs L/km:\n","df['highway-mpg'] = 235/df['highway-mpg']\n","\n","#creating 'bins' descrete sections of continuous data:\n","bins = np.linspace(min(df[\"horsepower\"]), max(df[\"horsepower\"]), 4) # create min max and 2 partitions between for total of 3 bins\n","group_names = ['Low', 'Medium', 'High'] #create the group names for the bins\n","df['horsepower-binned'] = pd.cut(df['horsepower'], bins, labels=group_names, include_lowest=True) #create columns for bins"]},{"cell_type":"markdown","metadata":{"id":"u2Zcxp5Wi--C"},"source":["### Filter, Sort, and Groupby\n","#### Use these commands to filter, sort, and group your data."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z8pVCMkti--D"},"outputs":[],"source":["df['drive-wheels'].unique() # Identifies unique values in df column\n","inputId = movies_df[movies_df['title'].isin(inputMovies['title'].tolist())] #selecting rows from a df that are in another df\n","df[df[col] > 0.5] # Rows where the column col is greater than 0.5\n","df[(df[col] > 0.5) & (df[col] < 0.7)] # Rows where 0.7 > col > 0.5\n","df.sort_values(col1) # Sort values by col1 in ascending order\n","df.sort_values(col2,ascending=False) # Sort values by col2 in descending order\n","df.sort_values([col1,col2],ascending=[True,False]) # Sort values by col1 in ascending order then col2 in descending order\n","df.groupby(col) # Returns a groupby object for values from one column\n","df.groupby([col1,col2]) # Returns groupby object for values from multiple columns\n","df.groupby(col1)[col2] # Returns the mean of the values in col2, grouped by the values in col1 \n","#(mean can be replaced with almost any function from the statistics module)\n","grouped_test2.get_group('4wd')['price'] # obtains values (not dataframe) of grouped data\n","\n","grouped_pivot = grouped_test1.pivot(index='column1',columns='body-style') #pivot the dataframe table by column1 in index\n","df.pivot_table(index=col1,values=[col2,col3],aggfunc=mean) # Create a pivot table that groups by col1 \n","#and calculates the mean of col2 and col3\n","\n","df.groupby(col1).agg(np.mean) # Find the average across all columns for every unique col1 group\n","df.apply(np.mean) # Apply the function np.mean() across each column\n","df.apply(np.max,axis=1) # Apply the function np.max() across each row"]},{"cell_type":"markdown","metadata":{"id":"gIizeyImi--D"},"source":["### Join/Combine\n","#### Use these commands to combine multiple dataframes into a single one."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gXjhkibyi--E"},"outputs":[],"source":["df1.append(df2) # Add the rows in df1 to the end of df2? (columns should be identical)\n","pd.concat([df1, df2],axis=1) # Add the columns in df1 to the end of df2 (rows should be identical)\n","df1.join(df2,on=col1,how='inner') # SQL-style join the columns in df1 with the columns on df2 where the rows \n","#for col have identical values. 'how' can be one of 'left', 'right', 'outer', 'inner'\n","inputMovies = pd.merge(inputId, inputMovies) # merging two df of equal length\n","topUsersRating=topUsers.merge(ratings_df, left_on='userId', right_on='userId', how='inner')"]},{"cell_type":"markdown","metadata":{"id":"B-sY407ci--E"},"source":["### Statistics\n","#### Use these commands to perform various statistical tests. (These can all be applied to a series as well.)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h7ruTpKei--E"},"outputs":[],"source":["df.describe() # Summary statistics for numerical columns\n","df.describe(include= \"all\" ) #Describe skips type objects, we can include these\n","df.mean() # Returns the mean of all columns\n","df.corr() # Returns the correlation between columns in a DataFrame\n","corr = dt.corr(method=\"pearson\") ## method : {‘pearson’, ‘kendall’, ‘spearman’}\n","df[[\"Col1\", \"Col2\"]].corr() #selecting specific columns for .corr()\n","df.count() # Returns the number of non-null values in each DataFrame column\n","df.max() # Returns the highest value in each column\n","df.min() # Returns the lowest value in each column\n","df.median() # Returns the median of each column\n","df.std() # Returns the standard deviation of each column"]},{"cell_type":"markdown","metadata":{"id":"S8QjVhiii--P"},"source":["#### Manual creation of a numpy array"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LKO94a3Oi--Q"},"outputs":[],"source":["row = np.array([0, 0, 1, 2, 2, 2])\n","col = np.array([0, 2, 2, 0, 1, 2])\n","data = np.array([1, 2, 3, 4, 5, 6])\n","csr_matrix((data, (row, col)), shape=(3, 3)).toarray()\n","array([[1, 0, 2],\n","       [0, 0, 3],\n","       [4, 5, 6]])"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"},"colab":{"name":"Reference_Pandas&someNumpy Cheatsheet.ipynb","provenance":[],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":0}